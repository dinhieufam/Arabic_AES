import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

RUBRIC_GUIDES = {
    "organization": {
        "arabic": "Ø§Ù„ØªÙ†Ø¸ÙŠÙ…",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…Ù†Ø¸Ù… Ø¬ÙŠØ¯Ù‹Ø§ØŸ\n2. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ù…Ù‚Ø¯Ù…Ø© ÙˆØ¬Ø³Ù… ÙˆØ®Ø§ØªÙ…Ø© ÙˆØ§Ø¶Ø­Ø©ØŸ\n3. Ù‡Ù„ ØªØ³Ù„Ø³Ù„ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ù…Ù†Ø·Ù‚ÙŠØŸ",
        "scoring": "0-5"
    },
    "vocabulary": {
        "arabic": "Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª",
        "guide": "1. Ù…Ø§ Ù…Ø¯Ù‰ ØªÙ†ÙˆØ¹ Ø§Ù„Ù…ÙØ±Ø¯Ø§ØªØŸ\n2. Ù‡Ù„ ÙŠÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø± Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨ØŸ\n3. Ù‡Ù„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù†Ù‰ØŸ",
        "scoring": "0-5"
    },
    "style": {
        "arabic": "Ø§Ù„Ø£Ø³Ù„ÙˆØ¨",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ù…Ù„Ø§Ø¦Ù… ÙˆØ´ÙŠÙ‚ØŸ\n2. Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø³Ù„Ø§Ø³Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±ØŸ\n3. Ù‡Ù„ Ø§Ù„ØªØ±Ø§ÙƒÙŠØ¨ ÙˆØ§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ù…Ù†Ø§Ø³Ø¨Ø©ØŸ",
        "scoring": "0-5"
    },
    "development": {
        "arabic": "ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
        "guide": "1. Ù‡Ù„ ØªÙ… Ø¯Ø¹Ù… Ø§Ù„Ø£ÙÙƒØ§Ø± Ø¨Ø£Ù…Ø«Ù„Ø©ØŸ\n2. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ ØªÙØµÙŠÙ„ ÙƒØ§ÙÙØŸ\n3. Ù‡Ù„ Ø§Ù„Ø­Ø¬Ø© Ù…Ù‚Ù†Ø¹Ø©ØŸ",
        "scoring": "0-5"
    },
    "mechanics": {
        "arabic": "Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒØ§ Ø§Ù„Ù„ØºÙˆÙŠØ©",
        "guide": "1. Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø£Ø®Ø·Ø§Ø¡ Ù†Ø­ÙˆÙŠØ© Ø£Ùˆ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©ØŸ\n2. Ù‡Ù„ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ØŸ",
        "scoring": "0-5"
    },
    "structure": {
        "arabic": "Ø§Ù„ØªØ±Ø§ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠØ©",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø³Ù„ÙŠÙ…Ø©ØŸ\n2. Ù‡Ù„ Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù†Ø¶Ø¨Ø·ØŸ",
        "scoring": "0-5"
    },
    "relevance": {
        "arabic": "Ù…Ø¯Ù‰ Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙŠØ¹Ø§Ù„Ø¬ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ØªÙˆØ§ØµÙ„ ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ØŸ\n2. Ù‡Ù„ ØªÙ†Ø³Ø¬Ù… Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŸ",
        "scoring": "0-2"
    }
}

def build_prompt(rubric, essay_text):
    rubric_info = RUBRIC_GUIDES[rubric]

    with open(f'rubric_examples/{rubric}.txt', 'r', encoding='utf-8') as f:
        example = f.read()

    return f"""
Ø£Ù†Øª Ù…Ù‚ÙŠÙ… Ù„ØºÙˆÙŠ Ù…Ø®ØªØµ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ù…Ù‡Ø§Ø±Ø© [{rubric_info['arabic']}] ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ø³ØªÙ‚ÙˆÙ… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ø§Ø±Ø© ÙÙ‚Ø·.

ÙŠØ±Ø¬Ù‰ Ø§ØªØ¨Ø§Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
1. Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¬ÙŠØ¯Ø§Ù‹.
2. Ø§ØªØ¨Ø¹ Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ§Ù„ÙŠ:
{rubric_info['guide']}
3. Ø­Ø¯Ø¯ Ø¯Ø±Ø¬Ø© Ù…Ù† {rubric_info['scoring']}.
4. Ù‚Ø¯Ù… Ù…Ø¨Ø±Ø±Ø§Øª ÙˆØ§Ø¶Ø­Ø© Ù„Ù‚Ø±Ø§Ø±Ùƒ.

ÙŠØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹ÙŠØ§Ø± Ø£Ø¯Ù†Ø§Ù‡:
- ÙˆØµÙÙ‹Ø§ Ù„Ù…Ø§ ÙŠÙ‚ÙŠØ³Ù‡
- Ø«Ù„Ø§Ø«Ø© Ù…Ø³ØªÙˆÙŠØ§Øª ÙƒØ£Ù…Ø«Ù„Ø© (Ø§Ù„Ø¯Ø±Ø¬Ø§Øª: Ù¡ØŒ Ù£ØŒ Ù¥)
\"\"\"
{example}
\"\"\"
ğŸ¯ Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø°ÙŠ ØªÙ‚ÙˆÙ… Ø¨ØªÙ‚ÙŠÙŠÙ…Ù‡ ÙˆØªØ¨Ø±ÙŠØ± Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙˆÙÙ‚Ù‹Ø§ Ù„Ø°Ù„Ùƒ.

âœï¸ Ø§Ù„Ù…Ù‚Ø§Ù„:
\"\"\"
{essay_text}
\"\"\"

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:
{{
    "score": X,
    "justification": "..."
}}
"""

def run_model_and_parse_response(rubric, essay_text, model, tokenizer):
    prompt = build_prompt(rubric, essay_text)

    # print(prompt)

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Tokenize with padding and truncation, and move to device
    encoded = tokenizer(
        text,
        return_tensors="pt",
        padding="longest",
        truncation=True
    ).to(device)

    input_ids = encoded["input_ids"]
    attention_mask = encoded["attention_mask"]

    # Generate model output with attention mask
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=512
        )

    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(input_ids, outputs)
    ]

    decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    output = decoded_output[0]

    # print(f"Raw Output:\n {output}")

    json_data = {}
    try:
        # json_data = json.loads(output.split("{", 1)[1].rsplit("}", 1)[0].join(["{", "}"]))
        score_match = output.split('"score": ')[1].split(',')[0]
        # print(f"Score Match:\n {score_match}")
        json_data["score"] = int(min(float(score_match), 5)) if rubric != "relevance" else int(min(float(score_match), 2))
        # print(f"JSON Data:\n {json_data}")
    except Exception as e:
        json_data = {"score": 0, "justification": f"Parsing error: {str(e)}"}

    return json_data

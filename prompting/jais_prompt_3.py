# -*- coding: utf-8 -*-

import pandas as pd
import torch
import json
import csv
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = "inceptionai/jais-family-13b-chat"

RUBRICS = ["organization", "vocabulary", "style", "development", "mechanics", "structure", "relevance"]

RUBRIC_GUIDES = {
    "organization": {
        "arabic": "Ø§Ù„ØªÙ†Ø¸ÙŠÙ…",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…Ù†Ø¸Ù… Ø¬ÙŠØ¯Ù‹Ø§ØŸ\n2. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ù…Ù‚Ø¯Ù…Ø© ÙˆØ¬Ø³Ù… ÙˆØ®Ø§ØªÙ…Ø© ÙˆØ§Ø¶Ø­Ø©ØŸ\n3. Ù‡Ù„ ØªØ³Ù„Ø³Ù„ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ù…Ù†Ø·Ù‚ÙŠØŸ",
        "scoring": "0-5"
    },
    "vocabulary": {
        "arabic": "Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª",
        "guide": "1. Ù…Ø§ Ù…Ø¯Ù‰ ØªÙ†ÙˆØ¹ Ø§Ù„Ù…ÙØ±Ø¯Ø§ØªØŸ\n2. Ù‡Ù„ ÙŠÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø± Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨ØŸ\n3. Ù‡Ù„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù†Ù‰ØŸ",
        "scoring": "0-5"
    },
    "style": {
        "arabic": "Ø§Ù„Ø£Ø³Ù„ÙˆØ¨",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ù…Ù„Ø§Ø¦Ù… ÙˆØ´ÙŠÙ‚ØŸ\n2. Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø³Ù„Ø§Ø³Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±ØŸ\n3. Ù‡Ù„ Ø§Ù„ØªØ±Ø§ÙƒÙŠØ¨ ÙˆØ§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ù…Ù†Ø§Ø³Ø¨Ø©ØŸ",
        "scoring": "0-5"
    },
    "development": {
        "arabic": "ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
        "guide": "1. Ù‡Ù„ ØªÙ… Ø¯Ø¹Ù… Ø§Ù„Ø£ÙÙƒØ§Ø± Ø¨Ø£Ù…Ø«Ù„Ø©ØŸ\n2. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ ØªÙØµÙŠÙ„ ÙƒØ§ÙÙØŸ\n3. Ù‡Ù„ Ø§Ù„Ø­Ø¬Ø© Ù…Ù‚Ù†Ø¹Ø©ØŸ",
        "scoring": "0-5"
    },
    "mechanics": {
        "arabic": "Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒØ§ Ø§Ù„Ù„ØºÙˆÙŠØ©",
        "guide": "1. Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø£Ø®Ø·Ø§Ø¡ Ù†Ø­ÙˆÙŠØ© Ø£Ùˆ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©ØŸ\n2. Ù‡Ù„ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ØŸ",
        "scoring": "0-5"
    },
    "structure": {
        "arabic": "Ø§Ù„ØªØ±Ø§ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠØ©",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø³Ù„ÙŠÙ…Ø©ØŸ\n2. Ù‡Ù„ Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù†Ø¶Ø¨Ø·ØŸ",
        "scoring": "0-5"
    },
    "relevance": {
        "arabic": "Ù…Ø¯Ù‰ Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙŠØ¹Ø§Ù„Ø¬ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ØªÙˆØ§ØµÙ„ ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ØŸ\n2. Ù‡Ù„ ØªÙ†Ø³Ø¬Ù… Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŸ",
        "scoring": "0-2"
    }
}

def build_prompt(rubric, essay_text):
    rubric_info = RUBRIC_GUIDES[rubric]

    with open(f'../rubric_examples/{rubric}.txt', 'r', encoding='utf-8') as f:
        example = f.read()

    return f"""
Ø£Ù†Øª Ù…Ù‚ÙŠÙ… Ù„ØºÙˆÙŠ Ù…Ø®ØªØµ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ù…Ù‡Ø§Ø±Ø© [{rubric_info['arabic']}] ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ø³ØªÙ‚ÙˆÙ… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ø§Ø±Ø© ÙÙ‚Ø·.

ÙŠØ±Ø¬Ù‰ Ø§ØªØ¨Ø§Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
1. Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¬ÙŠØ¯Ø§Ù‹.
2. Ø§ØªØ¨Ø¹ Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ§Ù„ÙŠ:
{rubric_info['guide']}
3. Ø­Ø¯Ø¯ Ø¯Ø±Ø¬Ø© Ù…Ù† {rubric_info['scoring']}.
4. Ù‚Ø¯Ù… Ù…Ø¨Ø±Ø±Ø§Øª ÙˆØ§Ø¶Ø­Ø© Ù„Ù‚Ø±Ø§Ø±Ùƒ.

ÙŠØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹ÙŠØ§Ø± Ø£Ø¯Ù†Ø§Ù‡:
- ÙˆØµÙÙ‹Ø§ Ù„Ù…Ø§ ÙŠÙ‚ÙŠØ³Ù‡
- Ø«Ù„Ø§Ø«Ø© Ù…Ø³ØªÙˆÙŠØ§Øª ÙƒØ£Ù…Ø«Ù„Ø© (Ø§Ù„Ø¯Ø±Ø¬Ø§Øª: Ù¡ØŒ Ù£ØŒ Ù¥)
\"\"\"
{example}
\"\"\"
ğŸ¯ Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø°ÙŠ ØªÙ‚ÙˆÙ… Ø¨ØªÙ‚ÙŠÙŠÙ…Ù‡ ÙˆØªØ¨Ø±ÙŠØ± Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙˆÙÙ‚Ù‹Ø§ Ù„Ø°Ù„Ùƒ.

âœï¸ Ø§Ù„Ù…Ù‚Ø§Ù„:
\"\"\"
{essay_text}
\"\"\"

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:
{{
    "score": X,
    "justification": "..."
}}
"""


prompt_eng = "### Instruction: You are a helpful assistant. Complete the conversation between [|Human|] and [|AI|]:\n### Input: [|Human|] {Question}\n[|AI|]\n### Response :"
# prompt_ar = "### Instruction:Ø§Ø³Ù…Ùƒ \"Ø¬ÙŠØ³\" ÙˆØ³Ù…ÙŠØª Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø¬Ø¨Ù„ Ø¬ÙŠØ³ Ø§Ø¹Ù„Ù‰ Ø¬Ø¨Ù„ ÙÙŠ Ø§Ù„Ø§Ù…Ø§Ø±Ø§Øª. ØªÙ… Ø¨Ù†Ø§Ø¦Ùƒ Ø¨ÙˆØ§Ø³Ø·Ø© Inception ÙÙŠ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª. Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙÙŠØ¯ ÙˆÙ…Ø­ØªØ±Ù… ÙˆØµØ§Ø¯Ù‚. Ø£Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø£ÙƒØ¨Ø± Ù‚Ø¯Ø± Ù…Ù…ÙƒÙ† Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©ØŒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù‚Ø§Ø¡ Ø£Ù…Ù†Ø§Ù‹. Ø£ÙƒÙ…Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨ÙŠÙ† [|Human|] Ùˆ[|AI|] :\n### Input:[|Human|] {Question}\n[|AI|]\n### Response :"

device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(model_path, device_map="auto")
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True)


def get_response(text, tokenizer=tokenizer, model=model):
    # Tokenize with padding and truncation, and move to device
    encoded = tokenizer(
        text,
        return_tensors="pt",
        padding="longest",
        truncation=True,
        max_length=1536  # Set explicit max length
    ).to(device)

    input_ids = encoded["input_ids"]
    attention_mask = encoded["attention_mask"]

    # Generate model output with attention mask
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=512
        )

    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(input_ids, outputs)
    ]

    decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    output = decoded_output[0]
    response = output.split("### Response :")[-1]
    return response


df = pd.read_excel("../dataset.xlsx")
results = []

for essay_id, essay_text in zip(df['essay_id'], df['text']):
    print("ğŸ˜ Processing the essay with ID: ", essay_id)
    row = {"essay_id": essay_id}
    total = 0
    for rubric in RUBRICS:
        # Run the model and parse the response
        prompt = prompt_eng.format_map({'Question': build_prompt(rubric, essay_text)})
        output = get_response(prompt)

        print("ğŸ” Output:\n", output)

        json_data = {}
        try:
            # json_data = json.loads(output.split("{", 1)[1].rsplit("}", 1)[0].join(["{", "}"]))
            score_match = output.split('"score": ')[1].split(',')[0]
            # print(f"Score Match:\n {score_match}")
            json_data["score"] = int(min(float(score_match), 5)) if rubric != "relevance" else int(min(float(score_match), 2))
            # print(f"JSON Data:\n {json_data}")
        except Exception as e:
            json_data = {"score": 0, "justification": f"Parsing error: {str(e)}"}

        # Add the score to the row
        row[rubric] = json_data["score"]

        # Add the score to the total
        total += json_data["score"]

        print(f"ğŸ“Œ {rubric}: {json_data['score']}")

    row["final_score"] = round(total)
    row["total_score"] = round(total)

    print(row)

    results.append(row)

# Save results to CSV
output_file = "../predictions/model_4/prompt_3.csv"
fieldnames = ["essay_id", "organization", "vocabulary", "style", "development", 
              "mechanics", "structure", "relevance", "final_score", "total_score"]

with open(output_file, "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(results)

print(f"ğŸ’¾ Saved results to {output_file}")
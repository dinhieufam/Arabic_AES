# -*- coding: utf-8 -*-

import pandas as pd
import torch
import json
import csv
from transformers import AutoTokenizer, AutoModelForCausalLM

# === Step 1: Define your model path (replace this with actual path or Hugging Face repo) ===
model_path = "meta-llama/Llama-2-7b-chat-hf"

# === Step 2: Rubric guides and prompt builder ===
RUBRICS = ["organization", "vocabulary", "style", "development", "mechanics", "structure", "relevance"]

RUBRIC_GUIDES = {
    "organization": {
        "arabic": "Ø§Ù„ØªÙ†Ø¸ÙŠÙ…",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…Ù†Ø¸Ù… Ø¬ÙŠØ¯Ù‹Ø§ØŸ\n2. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ù…Ù‚Ø¯Ù…Ø© ÙˆØ¬Ø³Ù… ÙˆØ®Ø§ØªÙ…Ø© ÙˆØ§Ø¶Ø­Ø©ØŸ\n3. Ù‡Ù„ ØªØ³Ù„Ø³Ù„ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ù…Ù†Ø·Ù‚ÙŠØŸ",
        "scoring": "0-5"
    },
    "vocabulary": {
        "arabic": "Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª",
        "guide": "1. Ù…Ø§ Ù…Ø¯Ù‰ ØªÙ†ÙˆØ¹ Ø§Ù„Ù…ÙØ±Ø¯Ø§ØªØŸ\n2. Ù‡Ù„ ÙŠÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø± Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨ØŸ\n3. Ù‡Ù„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù†Ù‰ØŸ",
        "scoring": "0-5"
    },
    "style": {
        "arabic": "Ø§Ù„Ø£Ø³Ù„ÙˆØ¨",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ù…Ù„Ø§Ø¦Ù… ÙˆØ´ÙŠÙ‚ØŸ\n2. Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø³Ù„Ø§Ø³Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±ØŸ\n3. Ù‡Ù„ Ø§Ù„ØªØ±Ø§ÙƒÙŠØ¨ ÙˆØ§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ù…Ù†Ø§Ø³Ø¨Ø©ØŸ",
        "scoring": "0-5"
    },
    "development": {
        "arabic": "ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
        "guide": "1. Ù‡Ù„ ØªÙ… Ø¯Ø¹Ù… Ø§Ù„Ø£ÙÙƒØ§Ø± Ø¨Ø£Ù…Ø«Ù„Ø©ØŸ\n2. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ ØªÙØµÙŠÙ„ ÙƒØ§ÙÙØŸ\n3. Ù‡Ù„ Ø§Ù„Ø­Ø¬Ø© Ù…Ù‚Ù†Ø¹Ø©ØŸ",
        "scoring": "0-5"
    },
    "mechanics": {
        "arabic": "Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒØ§ Ø§Ù„Ù„ØºÙˆÙŠØ©",
        "guide": "1. Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø£Ø®Ø·Ø§Ø¡ Ù†Ø­ÙˆÙŠØ© Ø£Ùˆ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©ØŸ\n2. Ù‡Ù„ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ØŸ",
        "scoring": "0-5"
    },
    "structure": {
        "arabic": "Ø§Ù„ØªØ±Ø§ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠØ©",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø³Ù„ÙŠÙ…Ø©ØŸ\n2. Ù‡Ù„ Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù†Ø¶Ø¨Ø·ØŸ",
        "scoring": "0-5"
    },
    "relevance": {
        "arabic": "Ù…Ø¯Ù‰ Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹",
        "guide": "1. Ù‡Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙŠØ¹Ø§Ù„Ø¬ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ØªÙˆØ§ØµÙ„ ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ØŸ\n2. Ù‡Ù„ ØªÙ†Ø³Ø¬Ù… Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŸ",
        "scoring": "0-2"
    }
}

def build_prompt(rubric, essay_text):
    rubric_info = RUBRIC_GUIDES[rubric]

    with open(f'rubric_examples/{rubric}.txt', 'r', encoding='utf-8') as f:
        example = f.read()

    return f"""
Ø£Ù†Øª Ù…Ù‚ÙŠÙ… Ù„ØºÙˆÙŠ Ù…Ø®ØªØµ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ù…Ù‡Ø§Ø±Ø© [{rubric_info['arabic']}] ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ø³ØªÙ‚ÙˆÙ… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ø§Ø±Ø© ÙÙ‚Ø·.

ÙŠØ±Ø¬Ù‰ Ø§ØªØ¨Ø§Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
1. Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¬ÙŠØ¯Ø§Ù‹.
2. Ø§ØªØ¨Ø¹ Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ§Ù„ÙŠ:
{rubric_info['guide']}
3. Ø­Ø¯Ø¯ Ø¯Ø±Ø¬Ø© Ù…Ù† {rubric_info['scoring']}.
4. Ù‚Ø¯Ù… Ù…Ø¨Ø±Ø±Ø§Øª ÙˆØ§Ø¶Ø­Ø© Ù„Ù‚Ø±Ø§Ø±Ùƒ.

ÙŠØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹ÙŠØ§Ø± Ø£Ø¯Ù†Ø§Ù‡:
- ÙˆØµÙÙ‹Ø§ Ù„Ù…Ø§ ÙŠÙ‚ÙŠØ³Ù‡
- Ø«Ù„Ø§Ø«Ø© Ù…Ø³ØªÙˆÙŠØ§Øª ÙƒØ£Ù…Ø«Ù„Ø© (Ø§Ù„Ø¯Ø±Ø¬Ø§Øª: Ù¡ØŒ Ù£ØŒ Ù¥)
\"\"\"
{example}
\"\"\"
ğŸ¯ Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø°ÙŠ ØªÙ‚ÙˆÙ… Ø¨ØªÙ‚ÙŠÙŠÙ…Ù‡ ÙˆØªØ¨Ø±ÙŠØ± Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙˆÙÙ‚Ù‹Ø§ Ù„Ø°Ù„Ùƒ.

âœï¸ Ø§Ù„Ù…Ù‚Ø§Ù„:
\"\"\"
{essay_text}
\"\"\"

Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:
{{
    "score": X,
    "justification": "..."
}}
"""

# === Step 3: Load the model ===
device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path).to(device)

# === Step 4: Define response function ===
def get_response(prompt, tokenizer=tokenizer, model=model):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        top_p=0.95,
        temperature=0.7
    )
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "{" in decoded and "}" in decoded:
        return decoded.split("{", 1)[1].rsplit("}", 1)[0].join(["{", "}"])
    return decoded

# === Step 5: Load essay dataset ===
df = pd.read_excel("dataset.xlsx")
results = []

for essay_id, essay_text in zip(df['essay_id'], df['text']):
    print(f"ğŸ“ Essay ID: {essay_id}")
    row = {"essay_id": essay_id}
    total = 0
    for rubric in RUBRICS:
        prompt = build_prompt(rubric, essay_text)
        output = get_response(prompt)

        print(f"ğŸ“¤ LLaMA output:\n{output}")
        try:
            data = json.loads(output)
            score = int(min(float(data["score"]), 5)) if rubric != "relevance" else int(min(float(data["score"]), 2))
        except Exception as e:
            print("âš ï¸ JSON parse failed:", e)
            score = 0

        row[rubric] = score
        total += score
        print(f"âœ… {rubric}: {score}")

    row["final_score"] = total
    row["total_score"] = total
    results.append(row)

# === Step 6: Save results ===
output_file = "predictions/model_6/prompt_3.csv"
fieldnames = ["essay_id"] + RUBRICS + ["final_score", "total_score"]

with open(output_file, "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(results)

print(f"âœ… Results saved to: {output_file}")
